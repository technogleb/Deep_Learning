{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение. Семинар и домашнее задание 1. Обучение полносвязной нейронной сети на numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "[__ Конспект с выводом формул__](https://github.com/nadiinchi/dl_labs/blob/master/nn_gradients.pdf)\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        ### your code here\n",
    "        self.input = input\n",
    "        self.output = np.maximum(input, np.zeros(shape=input.shape))\n",
    "        return self.output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        \n",
    "        #self.output[bool(self.output)] = 1\n",
    "        \n",
    "        return grad_output * np.sign(self.output), []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  2.,  3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weights) + self.biases\n",
    "        return self.output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_param_w = np.dot(grad_output.T, self.input).T\n",
    "        grad_param_b = np.dot(grad_output.T, np.ones(grad_output.shape[0]))\n",
    "        grad_params = np.r_[grad_param_w.ravel(), grad_param_b]\n",
    "        \n",
    "        return grad_input, grad_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "### your code here\\\n",
    "relu = ReLU()\n",
    "y = relu.forward\n",
    "def relu_sum(x):\n",
    "    return np.sum(y(x))\n",
    "\n",
    "# df/dx = df/dy * dy/dx; df/dy - grad_ouput (np.ones())\n",
    "grad_output = np.ones(shape=points.shape)\n",
    "\n",
    "relu.forward(points)\n",
    "grads  = relu.backward(grad_output)[0]\n",
    "numeric_grads = [eval_numerical_gradient(relu_sum, x) for x in points]\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "l = Dense(12, 32,)\n",
    "### your code here\n",
    "y = l.forward\n",
    "def dense_sum(x):\n",
    "    return np.sum(y(x))\n",
    "\n",
    "\n",
    "output = l.forward(x)\n",
    "grad_output = np.ones(shape=output.shape)\n",
    "grads = l.backward(grad_output)[0]\n",
    "numeric_grads = [eval_numerical_gradient(dense_sum, x) for x in points] \n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        # L = log(exp(x)/sum(exp(x))) = (log(exp(x)) - logsumexp(x)) = x - logsumexp(x)\n",
    "        self.input = input\n",
    "        self.output = self.input - logsumexp(3, axis=1, keepdims=True)\n",
    "        exp = np.exp(self.input)\n",
    "        self.softmax = exp / np.sum(exp, axis = 1, keepdims=True)\n",
    "        \n",
    "        return self.output\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        \n",
    "        return grad_output + self.softmax, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    #crossentropy is sum(y * log(softmax)). log(softmax) is what this function gets as an input from softmax output \n",
    "    \n",
    "    return -np.sum(activations * target)\n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    \n",
    "    return -target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "target = np.arange(10)\n",
    "target_encoded = np.eye(points.shape[0], points.shape[1])\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax_output = softmax.forward(points)\n",
    "softmax_crossentropy = grad_crossentropy(softmax_output, target_encoded)\n",
    "\n",
    "### your code here\n",
    "grads = softmax.backward(softmax_crossentropy)[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: crossentropy(softmax.forward(x), target_encoded), points)\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка и обучение нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    input = X\n",
    "    for layer in network:\n",
    "        output = layer.forward(input)\n",
    "        input = output\n",
    "    \n",
    "    return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_proba(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    input = X\n",
    "    for layer in network:\n",
    "        output = layer.forward(input)\n",
    "        input = output\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    In general, the optimization problems are of the form::\n",
      "    \n",
      "        minimize f(x) subject to\n",
      "    \n",
      "        g_i(x) >= 0,  i = 1,...,m\n",
      "        h_j(x)  = 0,  j = 1,...,p\n",
      "    \n",
      "    where x is a vector of one or more variables.\n",
      "    ``g_i(x)`` are the inequality constraints.\n",
      "    ``h_j(x)`` are the equality constrains.\n",
      "    \n",
      "    Optionally, the lower and upper bounds for each element in x can also be\n",
      "    specified using the `bounds` argument.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        Objective function.\n",
      "    x0 : ndarray\n",
      "        Initial guess.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (Jacobian, Hessian).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : bool or callable, optional\n",
      "        Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the\n",
      "        gradient will be estimated numerically.\n",
      "        `jac` can also be a callable returning the gradient of the\n",
      "        objective. In this case, it must accept the same arguments as `fun`.\n",
      "    hess, hessp : callable, optional\n",
      "        Hessian (matrix of second-order derivatives) of objective function or\n",
      "        Hessian of objective function times an arbitrary vector p.  Only for\n",
      "        Newton-CG, dogleg, trust-ncg.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  If neither `hess` nor\n",
      "        `hessp` is provided, then the Hessian product will be approximated\n",
      "        using finite differences on `jac`. `hessp` must compute the Hessian\n",
      "        times an arbitrary vector.\n",
      "    bounds : sequence, optional\n",
      "        Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
      "        ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the bounds on that parameter. Use None for one of ``min`` or\n",
      "        ``max`` when there is no bound in that direction.\n",
      "    constraints : dict or sequence of dict, optional\n",
      "        Constraints definition (only for COBYLA and SLSQP).\n",
      "        Each constraint is defined in a dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "        current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector.\n",
      "    \n",
      "    **Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot_encode(X, y):\n",
    "    y_encoded = np.zeros((X.shape[0], np.unique(y).shape[0]))\n",
    "    y_encoded[np.arange(X.shape[0]), y] = 1\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    ### your code here\n",
    "    set_weights(weights, network)\n",
    "    \n",
    "    y_proba = predict_proba(network, X)\n",
    "    y_encoded = onehot_encode(X, y)\n",
    "    \n",
    "    loss = crossentropy(y_proba, y_encoded)\n",
    "    \n",
    "    grad = []\n",
    "    grad_output = grad_crossentropy(y_proba, y_encoded)\n",
    "    for layer in reversed(network):\n",
    "        grad_output, grad_param = layer.backward(grad_output)\n",
    "        #print(len(grad_param))\n",
    "        grad.append(grad_param)\n",
    "    \n",
    "    grad = np.hstack((gr for gr in reversed(grad)))\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.43989191e-03,  -7.42899652e-03,   2.80465004e-03, ...,\n",
       "        -1.39378861e+00,  -3.30265148e+00,  -7.03491063e-01])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test. Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n"
     ]
    }
   ],
   "source": [
    "### your code here\n",
    "set_weights(res['x'], network)\n",
    "y_train_pred = predict(network, X_train)\n",
    "y_test_pred = predict(network, X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('train score: {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('test score: {}'.format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        set_weights(weights, self.network)\n",
    "        y_train_pred = predict(network, X_train)\n",
    "        y_test_pred = predict(network, X_test)\n",
    "        train_score = accuracy_score(y_train, y_train_pred)\n",
    "        test_score = accuracy_score(y_test, y_test_pred)\n",
    "        self.train_acc.append(train_score)\n",
    "        self.test_acc.append(test_score)\n",
    "        if self.print:\n",
    "            print('train score: {}'.format(train_score))\n",
    "            print('test score: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.14847809948032664\n",
      "test score: 0.12444444444444444\n",
      "train score: 0.2383073496659243\n",
      "test score: 0.19111111111111112\n",
      "train score: 0.26948775055679286\n",
      "test score: 0.2288888888888889\n",
      "train score: 0.20786933927245732\n",
      "test score: 0.21777777777777776\n",
      "train score: 0.24350408314773572\n",
      "test score: 0.2222222222222222\n",
      "train score: 0.22420193021529325\n",
      "test score: 0.19333333333333333\n",
      "train score: 0.24647364513734224\n",
      "test score: 0.22666666666666666\n",
      "train score: 0.2702301410541945\n",
      "test score: 0.25333333333333335\n",
      "train score: 0.370452858203415\n",
      "test score: 0.3422222222222222\n",
      "train score: 0.44097995545657015\n",
      "test score: 0.37555555555555553\n",
      "train score: 0.47438752783964366\n",
      "test score: 0.42444444444444446\n",
      "train score: 0.5352635486265775\n",
      "test score: 0.4911111111111111\n",
      "train score: 0.6636971046770601\n",
      "test score: 0.6133333333333333\n",
      "train score: 0.682256867112101\n",
      "test score: 0.6222222222222222\n",
      "train score: 0.7178916109873794\n",
      "test score: 0.6644444444444444\n",
      "train score: 0.7223459539717891\n",
      "test score: 0.6711111111111111\n",
      "train score: 0.7223459539717891\n",
      "test score: 0.6822222222222222\n",
      "train score: 0.74090571640683\n",
      "test score: 0.7066666666666667\n",
      "train score: 0.7750556792873051\n",
      "test score: 0.7288888888888889\n",
      "train score: 0.7906458797327395\n",
      "test score: 0.7777777777777778\n",
      "train score: 0.7706013363028953\n",
      "test score: 0.7511111111111111\n",
      "train score: 0.8017817371937639\n",
      "test score: 0.78\n",
      "train score: 0.8032665181885672\n",
      "test score: 0.7777777777777778\n",
      "train score: 0.8210838901262064\n",
      "test score: 0.8\n",
      "train score: 0.8470675575352635\n",
      "test score: 0.8266666666666667\n",
      "train score: 0.8626577579806979\n",
      "test score: 0.8333333333333334\n",
      "train score: 0.874536005939124\n",
      "test score: 0.8511111111111112\n",
      "train score: 0.8775055679287305\n",
      "test score: 0.8577777777777778\n",
      "train score: 0.8804751299183371\n",
      "test score: 0.86\n",
      "train score: 0.8752783964365256\n",
      "test score: 0.8555555555555555\n",
      "train score: 0.8893838158871566\n",
      "test score: 0.8755555555555555\n",
      "train score: 0.9005196733481812\n",
      "test score: 0.8866666666666667\n",
      "train score: 0.9086859688195991\n",
      "test score: 0.8955555555555555\n",
      "train score: 0.9146250927988122\n",
      "test score: 0.9044444444444445\n",
      "train score: 0.9213066072754269\n",
      "test score: 0.9088888888888889\n",
      "train score: 0.9265033407572383\n",
      "test score: 0.9088888888888889\n",
      "train score: 0.9250185597624351\n",
      "test score: 0.9111111111111111\n",
      "train score: 0.9294729027468448\n",
      "test score: 0.9133333333333333\n",
      "train score: 0.9376391982182628\n",
      "test score: 0.9155555555555556\n",
      "train score: 0.9406087602078693\n",
      "test score: 0.92\n",
      "train score: 0.9406087602078693\n",
      "test score: 0.9222222222222223\n",
      "train score: 0.9465478841870824\n",
      "test score: 0.9288888888888889\n",
      "train score: 0.9517446176688938\n",
      "test score: 0.9266666666666666\n",
      "train score: 0.9576837416481069\n",
      "test score: 0.9444444444444444\n",
      "train score: 0.9561989606533037\n",
      "test score: 0.9333333333333333\n",
      "train score: 0.9606533036377134\n",
      "test score: 0.9355555555555556\n",
      "train score: 0.96362286562732\n",
      "test score: 0.94\n",
      "train score: 0.9673348181143281\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9695619896065331\n",
      "test score: 0.9533333333333334\n",
      "train score: 0.9740163325909429\n",
      "test score: 0.9533333333333334\n",
      "train score: 0.977728285077951\n",
      "test score: 0.9466666666666667\n",
      "train score: 0.9799554565701559\n",
      "test score: 0.94\n",
      "train score: 0.9784706755753526\n",
      "test score: 0.9333333333333333\n",
      "train score: 0.9814402375649591\n",
      "test score: 0.94\n",
      "train score: 0.9814402375649591\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9844097995545658\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9821826280623608\n",
      "test score: 0.9444444444444444\n",
      "train score: 0.9844097995545658\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9844097995545658\n",
      "test score: 0.9488888888888889\n",
      "train score: 0.985894580549369\n",
      "test score: 0.9533333333333334\n",
      "train score: 0.9866369710467706\n",
      "test score: 0.9466666666666667\n",
      "train score: 0.9903489235337788\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9888641425389755\n",
      "test score: 0.94\n",
      "train score: 0.991833704528582\n",
      "test score: 0.94\n",
      "train score: 0.994060876020787\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9948032665181886\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9933184855233853\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9948032665181886\n",
      "test score: 0.9466666666666667\n",
      "train score: 0.9955456570155902\n",
      "test score: 0.9422222222222222\n",
      "train score: 0.9977728285077951\n",
      "test score: 0.9488888888888889\n",
      "train score: 0.9970304380103935\n",
      "test score: 0.9466666666666667\n",
      "train score: 0.9977728285077951\n",
      "test score: 0.9488888888888889\n",
      "train score: 0.9970304380103935\n",
      "test score: 0.9533333333333334\n",
      "train score: 0.9977728285077951\n",
      "test score: 0.9555555555555556\n",
      "train score: 0.9977728285077951\n",
      "test score: 0.9555555555555556\n",
      "train score: 0.9992576095025983\n",
      "test score: 0.9577777777777777\n",
      "train score: 0.9992576095025983\n",
      "test score: 0.9577777777777777\n",
      "train score: 0.9992576095025983\n",
      "test score: 0.96\n",
      "train score: 0.9992576095025983\n",
      "test score: 0.9577777777777777\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9488888888888889\n",
      "train score: 1.0\n",
      "test score: 0.9488888888888889\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9488888888888889\n",
      "train score: 1.0\n",
      "test score: 0.9488888888888889\n",
      "train score: 1.0\n",
      "test score: 0.9488888888888889\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9555555555555556\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9533333333333334\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n",
      "train score: 1.0\n",
      "test score: 0.9511111111111111\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23042f1bf60>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230481a1dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты с числом слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 5))\n",
    "accs_test = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(X = X_train, n_layers = 2, hidden_layers_size = 32):\n",
    "    \n",
    "    network = []\n",
    "    network.append(Dense(X.shape[1], hidden_layers_size))\n",
    "    network.append(ReLU())\n",
    "    hidden_layers_size = hidden_layers_size\n",
    "    for i in range(n_layers):\n",
    "        network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "    network.append(Dense(hidden_layers_size, 10))\n",
    "    network.append(Softmax())\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g.sinyakov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\g.sinyakov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "### your code here\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        network = create_network(n_layers=i)\n",
    "        weights = get_weights(network)\n",
    "        \n",
    "        cb = Callback(network, X_train, y_train, X_test, y_test)\n",
    "        res = minimize(compute_loss_grad, weights,\n",
    "                       args=[network, X_train, y_train],\n",
    "                       method=\"L-BFGS-B\",\n",
    "                       jac=True,\n",
    "                       callback=cb.call)\n",
    "        train_score = max(cb.train_acc)\n",
    "        test_score = max(cb.test_acc)\n",
    "        accs_train[i, j] = train_score\n",
    "        accs_test[i, j] = test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x230480625c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF5VJREFUeJzt3XuYJXV95/H3ZwYRw3CZyYwRGRQ0EIMaRVsUSbyyLHiB\nXYMK0WRVAuojatSYxU1EJHlcZR/jJbKro6LBC4iSy0RxId5iMFxmRhAFlnUCKCMo43IHRWG++8ep\nrjk2Pd3VQ5+unun363nO06fq/KrqewrmfKp+dUtVIUkSwKK+C5AkzR+GgiSpZShIklqGgiSpZShI\nklqGgiSpZShI00jymiRfad4/OMmdSR4+S/P+9yQHzca8pNlgKGhWNT+Y469NSX42NPyyBzDfi5K8\nfDZr3RpVdU9VLamqG5q6zkryFw9gfo+uqgu3ZtokP05y99D6/aetrUMat0PfBWj7UlVLxt8nuQ74\n46r6Sn8VbfcOraoLZjpRksVVdd8oCtK2zT0Fzakki5O8Pck1SX6a5DNJdm8+27nZ8r45ya1JLk6y\nNMl7gacAH2u2iN+7hXkfm+SHSTYmeWuzJf27zWe/skWf5LAk64eGT0pybZI7knwvyfO3sIydklSS\nlUneAPw+8Pamrs833+0zE6b5aJJ3b2F+wzW+u1kfZzZ1XJ7kiTNZv1vSfP8PJjk/yV3AQRP3viZ0\nk41/z+OaLq5bkrxvqO1jklyQ5LZmfZ8xG3Wqf4aC5tpbgUOB3wVWAr8Exn9s/pjB3uuewHLgBOAX\nVfUWYA2DvY4lzfCvaH483w+8tJnv3s08uroaeDqwG/Ae4KwkU05fVR8EzgH+sqnrxcAZwBFJljR1\nPRg4CvhUxzr+M3A6sDvw1eY7TeULSW5K8uUkj52m7cuBtwO7MFifXRwOHAA8CXhlkmc14/878A9N\nnY8APtJxfprnDAXNtVcDJ1bVDVX1c+CdwEuThEFArAAeXVX3VtWaqrqr43xfApxTVRdW1T3Af2MG\n/39X1eeq6saq2lRVnwJ+BDx5Jl+smc8PgLUMftwBXghcW1VXdJzF16rqn5uunU8BU+0pHMUg/PYB\nLgbOS7LLFO2/UFUXN9/xno71vKuqbq+qa4FvDtXzy2bZD6uqn1XVtzrOT/OcoaA50/zw7wWc23QP\n3QpcyuD/w18HPg78C4Ot3w1J3pVkccfZPxy4fnygqm4DbptBbcc23TXjdf0mM9vTGPa3DLbKaf52\n3UsA+PHQ+7uBJVtqWFUXVNXPq+quqjoZuBd42hTzvn6Kz2Zaz5uAXwMubdZb7ycBaHYYCpozNbgl\n74+A51TV7kOvnarqp82ZPSdV1WOAZwAvBo4en3ya2d/IIHAASLIbg66gcXcx+BEb97ChtvsBfwMc\nDyyrqt2B9UC6fK1Jxn0BeFrTnXMocGaH+cyGYuqaJ9a6xXUy7YKqflRVrwL2AN4AnJ7kEV2n1/xl\nKGiufRh4d5K9AJI8NMkLm/eHJNk/ySLgdgZbvuNnyPwEeNQU8z0beFGSpzb9+H8FbBr6/DLgBUl2\nT7In8Pqhz5Y0bTcCi5K8hsGeQhf3q6uq7gRWMwiDb1TVjyeb8IFI8qgkByV5UJKHNAfRd2LQjdTV\nZcBRzUHlxwCvmMHyX5rk4U3Q39qMvncGy9Y8ZShorp0KfAX4WpI7gH9jcBATBgeY/xG4A/gecC6D\nH3sYHIz+o+YsmFMnzrSqLgXewmArfQPwQ+CnQ01OZ7D1/0PgiwxtvVfVtxmE1VoGexz7NO+7WAU8\npel2Omto/N8Cj2dmXUczsSvwUeAWBt/3GcDhTbdZV6cyOLC/kcH3+PQMpj0IWJfkTuDzwPHj125o\n2xYfsqPtVZIfA0dtzXn8s7Ds/RgEy8Oq6u65Xr60tdxTkGZZc3D8zcCnDQRta7yiWZpFSZYx6KK6\nBviPPZcjzZjdR5Kklt1HkqTWyLqPkpwOvAC4qaoeN8nnAT4API/BRTGvaM4CmdLy5ctr7733nuVq\nJWn7tm7dup9W1Yrp2o3ymMIngQ8xuBfMZA4H9m1eTwX+V/N3SnvvvTdr13Y9W1CSBJDkB13ajaz7\nqKq+Cdw8RZMjgTNq4CJg9yR7jKoeSdL0+jymsCe/ei+WDc24+0lyfJK1SdZu3LhxToqTpIWoz1CY\n7B4tk54KVVWrqmqsqsZWrJi2S0yStJX6DIUNDN3AjME98L1MXpJ61GcorGZwL5skeRpwW1Xd2GM9\nkrTgjfKU1DOBZwHLk2wA3gE8CKCqPszgZmfPY3CTsruBV46qFklSNyMLhao6ZprPC3jdqJYvSZo5\nr2iWJLUWZCgsW7aMJL2+li1b1vdqkKT7WZB3Sb35DfcxeEZJn+6bvok0Tw3uUvPAeUPO+WdBhgIn\nz+ThVNu5k3ebvs1c2Ab+myykH8Jly5Zxyy23jHw5U63TpUuXcvPNU90UQaOwMENBrbzz9t5/pJJQ\nJ/daAuAP4TD3phcuQ0GztgW8tZYuXdrr8sfdcsst8yIg5wM3FhYuQ2GB6/sfvqT5xVCQGvWOXXs/\nxlLv6LvLRgudoSA17DKRFuh1CpKkyRkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJ\nahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkK\nkqSWoSBJahkKkqSWoSBJahkKkqTWSEMhyWFJrk6yPsmJk3z+iCRfT3JpksuTPG+U9UiSpjayUEiy\nGDgNOBzYHzgmyf4Tmv0FcHZVHQAcDfzPUdUjSZreKPcUDgTWV9U1VfUL4CzgyAltCti1eb8bcMMI\n65EkTWOHEc57T+D6oeENwFMntDkZOD/J64GdgUNGWI8kaRqj3FPIJONqwvAxwCeraiXwPOBTSe5X\nU5Ljk6xNsnbjxo0jKFWSBKMNhQ3AXkPDK7l/99CxwNkAVXUhsBOwfOKMqmpVVY1V1diKFStGVK4k\naZShsAbYN8k+SXZkcCB59YQ2PwSeC5DktxmEgrsCktSTkYVCVd0LnACcB1zF4CyjK5KckuSIptlb\ngOOSfAc4E3hFVU3sYpIkzZFRHmimqs4Fzp0w7qSh91cCB4+yBklSd17RLElqGQqSpJahIElqGQqS\npJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJah\nIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqTRsKSXafi0IkSf3rsqewLsmZSQ4deTWSpF51\nCYV9gTOA45J8P8kpSR494rokST2YNhSqalNVfbmqXgwcBxwLXJbkq0kOHHmFkqQ5s8N0DZpjCi8D\n/gi4BXgT8PfAk4HPAfuMskBJ0tyZNhSANcBngZdU1Q+Gxl+U5KOjKUuS1IcuofBbVbVpsg+q6l2z\nXI8kqUddDjSfO3xaapKlSb40wpokST3pEgoPq6pbxweq6hbg4aMrSZLUly6hcF+SleMDSR4xwnok\nST3qckzhJOBbSb7WDD8beO3oSpIk9WXaUKiqLzXXIxwEBPivVXXTyCuTJM25rjfE+znwQ+AnwG8m\nefroSpIk9aXLDfFeBfwb8DXgPc3fTqeiJjksydVJ1ic5cQttXpLkyiRXJPnsDGqXJM2yLnsKbwLG\ngOuq6vcYXMl843QTJVkMnAYcDuwPHJNk/wlt9gXeBhxcVY8F/mRm5UuSZlOXUPh5Vf0MIMmOVXUF\n8JgO0x0IrK+qa6rqF8BZwJET2hwHnNac5orHKiSpX11C4cbm4rV/As5Lcg6DYwvT2RO4fmh4QzNu\n2H7Afkm+leSiJIdNNqMkxydZm2Ttxo0bOyxakrQ1upx9dETz9u1JngvsBnS5ojmTzW6S5e8LPAtY\nCfxrkscNXyzX1LAKWAUwNjY2cR6SpFkyZSg0xwW+XVVPAKiqr85g3huAvYaGVwI3TNLmoqr6JXBt\nkqsZhMSaGSxHkjRLpuw+qqr7gCuTTOz26WINsG+SfZLsCBwNrJ7Q5h8YXAxHkuUMupOu2YplSZJm\nQZcrmpcDVyW5ELhrfGRVvWiqiarq3iQnAOcBi4HTq+qKJKcAa6tqdfPZoUmuBO4D3lpV/28rv4sk\n6QFK1dRd9M1xhPuZYVfSrBkbG6u1a9f2sWht55Iw3b+HhVDDfKljPtSwPUmyrqrGpmvX5UBzLz/+\nkqS51+VxnHew+ayhHRh0Bd1TVbuOsjBJ0tzrsqewy/j7JIuAFwFPGGVRkqR+dL0hHgBVtamqvgD8\nhxHVI0nqUZfuoyOGBhcxuA/SZBemSZK2cV1OSX3x0Pt7geu4/z2MJEnbgS7HFP5wLgqRJPWvy/MU\nPt7cEG98eGmSj462LElSH7ocaH7S8A3qmttcP3l0JUmS+tIlFBYl2W18IMlS4EGjK0mS1JcuB5rf\nD1yY5HMMLmI7Gjh1pFVJknrR5UDzJ5KsA57D4FTUl1bVd0demSRpznW5TuEpwFVVdXkzvEuSsary\nrnSStJ3pckxhFXD30PBdwEdGU44kqU+dDjRX1abxgea9B5olaTvUJRSuTfLaJIuTLEryOgZXNUuS\ntjNdQuHVwHOBnzSvZwLHjbIoSVI/upx99BPgqDmoRZLUsy5nHz0YeAXwWGCn8fFVdfzoypIk9aFL\n99EZwN7AC4CLgUcDPx9hTZKknnQJhf2q6m3AnVX1ceAw4HGjLUuS1IcuofDL5u+tSX4b2AV45OhK\nkiT1pcu9jz7e3ATvHcB5wK8BJ420KklSL7qcfTR+9fLXgUeMthxJUp+6dB9JkhYIQ0GS1OryOM77\ndTFNNk6StO3rsqdwScdxkqRt3Ba3+JM8FNgDeEiSxzN4wA7ArgzOQJIkbWem6gZ6PvAqYCVwGptD\n4Q7g7SOuS5LUgy2GQlV9AvhEkpdU1dlzWJMkqSddjik8NMmuAEk+nOSSJM8dcV2SpB50CYXjq+r2\nJIcy6Ep6LXDqaMuSJPWhSyhU8/dw4BNVta7jdJKkbUyXH/fvJDkXeCHw5SRL2BwUkqTtSJeL0F4J\nPBlYX1V3J1kOHDvasiRJfZh2T6Gq7gMexeBYAsBDukwHkOSwJFcnWZ/kxCnaHZWkkox1ma8kaTS6\n3ObiQ8CzgZc3o+4CPtxhusUMrm84HNgfOCbJ/pO02wV4A4OnukmSetRli//pVfVqmkdwVtXNwI4d\npjuQQZfTNVX1C+As4MhJ2v0lg7OZfMSnJPWs05PXkiyiObic5NeBTR2m2xO4fmh4QzOuleQAYK+q\n+uJUM0pyfJK1SdZu3Lixw6IlSVtji6EwdCfU04BzgBVJ3glcALynw7wzybj2rKUmaN4HvGW6GVXV\nqqoaq6qxFStWdFi0JGlrTHX20SXAk6rqjCTrgEMY/NC/uKq+12HeG4C9hoZXAjcMDe8CPA74RhKA\nhwGrkxxRVWtn8B0kSbNkqlBot/Sr6grgihnOew2wb5J9gB8BRwN/MDTP24Dl7cKSbwB/aiBIUn+m\nCoUVSd68pQ+r6q+nmnFV3ZvkBOA8YDFwelVdkeQUYG1Vrd6qiiVJIzNVKCwGljD5sYFOqupc4NwJ\n407aQttnbe1yJEmzY6pQuLGqTpmzSiRJvZvqlNSt3kOQJG2bpgoFn5kgSQvMFkOhuXJZkrSA+FwE\nSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLL\nUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAk\ntQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktUYaCkkOS3J1kvVJTpzk8zcnuTLJ5Um+muSRo6xH\nkjS1kYVCksXAacDhwP7AMUn2n9DsUmCsqn4H+AJw6qjqkSRNb5R7CgcC66vqmqr6BXAWcORwg6r6\nelXd3QxeBKwcYT2SpGmMMhT2BK4fGt7QjNuSY4EvT/ZBkuOTrE2yduPGjbNYoiRp2A4jnHcmGVeT\nNkxeDowBz5zs86paBawCGBsbm3QekmZXMtk/4bmzdOnSXpe/UI0yFDYAew0NrwRumNgoySHAnwPP\nrKp7RliPpI6qHti2V5IHPA/1Y5TdR2uAfZPsk2RH4Ghg9XCDJAcAHwGOqKqbRliLJKmDkYVCVd0L\nnACcB1wFnF1VVyQ5JckRTbP/ASwBPp/ksiSrtzA7SdIcGGX3EVV1LnDuhHEnDb0/ZJTLlyTNjFc0\nS5JahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJaI72iWdrWeGdQLXSGgtTwrp6S\noSBpK3TZo+rSxiCefwwFSTPmj/n2ywPNkqSWoSBJahkKkqSWoSBJahkKkqSWoSBJahkKkqSW1ylI\n0lRO3q3vCgZOvm1OFmMoSNJU5ujHeL6w+0iS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIU\nJEmtbGtPUEqyEfhB33UAy4Gf9l3EPOG6GHA9bOa62Gy+rItHVtWK6Rptc6EwXyRZW1VjfdcxH7gu\nBlwPm7kuNtvW1oXdR5KklqEgSWoZCltvVd8FzCOuiwHXw2aui822qXXhMQVJUss9BUlSy1CQJLUM\nhRlIcnqSm5J8r+9a+pZkryRfT3JVkiuSvLHvmvqSZKcklyT5TrMu3tl3TX1LsjjJpUm+2HctfUpy\nXZLvJrksydq+6+nCYwozkOQZwJ3AGVX1uL7r6VOSPYA9qurbSXYB1gH/qaqu7Lm0OZckwM5VdWeS\nBwEXAG+sqot6Lq03Sd4MjAG7VtUL+q6nL0muA8aqaj5cvNaJewozUFXfBG7uu475oKpurKpvN+/v\nAK4C9uy3qn7UwJ3N4IOa14Ld2kqyEng+8LG+a9HMGQp6wJLsDRwAXNxvJf1puksuA24C/rmqFuy6\nAN4P/Bmwqe9C5oECzk+yLsnxfRfThaGgByTJEuAc4E+q6va+6+lLVd1XVU8EVgIHJlmQ3YtJXgDc\nVFXr+q5lnji4qp4EHA68rumCntcMBW21pv/8HOAzVfV3fdczH1TVrcA3gMN6LqUvBwNHNH3pZwHP\nSfLpfkvqT1Xd0Py9Cfh74MB+K5qeoaCt0hxc/ThwVVX9dd/19CnJiiS7N+8fAhwC/J9+q+pHVb2t\nqlZW1d7A0cDXqurlPZfViyQ7NydhkGRn4FBg3p+5aCjMQJIzgQuB30qyIcmxfdfUo4OBP2SwJXhZ\n83pe30X1ZA/g60kuB9YwOKawoE/FFAC/AVyQ5DvAJcCXqup/91zTtDwlVZLUck9BktQyFCRJLUNB\nktQyFCRJLUNBktQyFLRdS1JJ3js0/KdJTh7Bcl6R5EOzPV9prhkK2t7dA7woyfK+C3kgkuzQdw1a\nGAwFbe/uZfCM3DdN/CDJJ5McNTR8Z/P3WUn+JcnZSf5vkncneVnzzITvJnn0VAtM8sIkFzfPE/hK\nkt9IsijJ95OsaNosSrI+yfLmiuhzkqxpXgc3bU5OsirJ+cAZSR7b1HBZksuT7DuL60kCDAUtDKcB\nL0uy2wymeQLwRuDxDK7c3q+qDmRwO+jXTzPtBcDTquoABvf/+bOq2gR8GnhZ0+YQ4DvNffY/ALyv\nqp4C/D6/esvpJwNHVtUfAK8BPtDceG8M2DCD7yN14i6ptntVdXuSM4A3AD/rONmaqroRIMm/A+c3\n478LPHuaaVcCn2seRLQjcG0z/nTgHxncWvpVwCea8YcA+w9uJwXAruP3zAFWV9V4zRcCf948r+Dv\nqur7Hb+L1Jl7Cloo3g8cC+w8NO5emn8DzQ3+dhz67J6h95uGhjcx/cbU3wAfqqrHA68GdgKoquuB\nnyR5DvBU4MtN+0XAQVX1xOa1Z/PgIoC7xmdaVZ8FjmAQbOc185FmlaGgBaGqbgbOZhAM465j0D0D\ncCSDJ6bNht2AHzXv/8uEzz7GoBvp7Kq6rxl3PnDCeIMkT5xspkkeBVxTVR8EVgO/M0v1Si1DQQvJ\ne4Hhs5A+CjwzySUMttzvmnSqmTsZ+HySfwUmPpt3NbCEzV1HMOjWGmsOHl/J4NjBZF4KfK95wttj\ngDNmqV6p5V1SpTmUZIzBQeXf67sWaTIeaJbmSJITgdey+Qwkad5xT0GS1PKYgiSpZShIklqGgiSp\nZShIklqGgiSp9f8BurOK6e8Vru8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23048005550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96444444,  0.95555556,  0.96666667,  0.96444444,  0.96666667],\n",
       "       [ 0.14666667,  0.96      ,  0.96      ,  0.95777778,  0.95111111],\n",
       "       [ 0.94      ,  0.95333333,  0.13333333,  0.13333333,  0.96      ],\n",
       "       [ 0.95777778,  0.96444444,  0.12222222,  0.19777778,  0.95333333],\n",
       "       [ 0.08444444,  0.08444444,  0.08444444,  0.08444444,  0.08444444]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "    Качество на первых двух слоях остается примерно одинаковым, но при дальнейшем увеличении слоев качество резко падает, как и     разброс возможных значений (устойчивость)\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "    Нет, так как при нулевом количестве доп слоев (по сути случай логистической регрессии), качество не хуже. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
