{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть материала украдена из курса \"Глубинное обучение\" ФКН ВШЭ https://www.hse.ru/ba/ami/courses/205504078.html, за что им большое спасибо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Актуальная версия этого ноутбука обретается по адресу\n",
    "https://github.com/nadiinchi/dl_labs/blob/master/lab_pytorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Устанавливаем pytorch\n",
    "\n",
    "## Linux/OSX\n",
    "\n",
    "\n",
    "На оффсайте http://pytorch.org/ надо выбрать подходящую конфигурацию и скачать.\n",
    "\n",
    "Версию python можно узнать в терминале:\n",
    "```\n",
    "python --version\n",
    "```\n",
    "\n",
    "\n",
    "## Windows without GPU\n",
    "\n",
    "Проще всего поставить при помощи конды:\n",
    "```\n",
    "conda install -c peterjc123 pytorch\n",
    "```\n",
    "\n",
    "## Windows with GPU\n",
    "\n",
    "Смотрите https://github.com/peterjc123/pytorch-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![img](https://s1.postimg.org/6fl45xnvnj/pytorch-logo-dark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "add 5 :\n",
      "[[ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]\n",
      " [17 18 19 20]]\n",
      "X*X^T  :\n",
      " [[ 14  38  62  86]\n",
      " [ 38 126 214 302]\n",
      " [ 62 214 366 518]\n",
      " [ 86 302 518 734]]\n",
      "mean over cols :\n",
      "[  1.5   5.5   9.5  13.5]\n",
      "cumsum of cols :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  6  8 10]\n",
      " [12 15 18 21]\n",
      " [24 28 32 36]]\n"
     ]
    }
   ],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n %s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of y: <class 'torch.IntTensor'>\n",
      "Type of y <class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "y = torch.from_numpy(x)\n",
    "print('Type of y:', type(y))\n",
    "y = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "print('Type of y', type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of x: <class 'torch.FloatTensor'>\n",
      "X :\n",
      "\n",
      "  0   1   2   3\n",
      "  4   5   6   7\n",
      "  8   9  10  11\n",
      " 12  13  14  15\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "add 5 :\n",
      "\n",
      "  5   6   7   8\n",
      "  9  10  11  12\n",
      " 13  14  15  16\n",
      " 17  18  19  20\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "X*X^T  :\n",
      " \n",
      "  14   38   62   86\n",
      "  38  126  214  302\n",
      "  62  214  366  518\n",
      "  86  302  518  734\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "mean over cols :\n",
      " \n",
      "  1.5000\n",
      "  5.5000\n",
      "  9.5000\n",
      " 13.5000\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "cumsum of cols :\n",
      " \n",
      "  0   1   2   3\n",
      "  4   6   8  10\n",
      " 12  15  18  21\n",
      " 24  28  32  36\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,16).view(4,4)\n",
    "print('Type of x:', type(x))\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", torch.matmul(x, x.transpose(1, 0)))\n",
    "print(\"mean over cols :\\n\", torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n\", torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy vs Pytorch\n",
    "\n",
    "Numpy и Pytorch не требуют описания статического графа вычислений. \n",
    "\n",
    "Можно отлаживаться с помощью pdb или просто print.\n",
    "\n",
    "API несколько различается:\n",
    "\n",
    "```\n",
    "x.reshape([1,2,8]) -> x.view(1,2,8)\n",
    "x.sum(axis=-1) -> x.sum(dim=-1)\n",
    "x.astype('int64') -> x.type(torch.LongTensor)\n",
    "```\n",
    "\n",
    "\n",
    "Легко конвертировать между собой:\n",
    "\n",
    "```\n",
    "torch.from_numpy(npx) -- вернет Tensor\n",
    "tt.numpy() -- вернет Numpy Array\n",
    "```\n",
    "\n",
    "\n",
    "Если что:\n",
    "- смотрите документацию\n",
    "- гуглите (Stackoverflow/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0, 2 * np.pi, 16)\n",
    "\n",
    "# Mini-task: compute a vector of sin^2(x) + cos^2(x)\n",
    "out = torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "print(out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 16]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда работаем с большими массивами, память надо экономить.\n",
    "Некоторые операции происходят с созданием нового объекта – результата вычислений,\n",
    "некоторые изменяют данный объект (in-place операции).\n",
    "В pytorch обычно эти операции различаются добавлением подчеркивания:\n",
    "```\n",
    "x.exp()   # not-in-place operation\n",
    "x.exp_()  # in-place operation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not-in-place:\n",
      "\tx.exp():\t\t [  1.           2.71828175   7.38905621  20.08553696]\n",
      "\tx:\t\t\t [ 0.  1.  2.  3.]\n",
      "In-place:\n",
      "\tx.exp_():\t\t [  1.           2.71828175   7.38905621  20.08553696]\n",
      "\tx after x.exp_():\t [  1.           2.71828175   7.38905621  20.08553696]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print('Not-in-place:')\n",
    "print('\\tx.exp():\\t\\t', x.exp().numpy())\n",
    "print('\\tx:\\t\\t\\t', x.numpy())\n",
    "print('In-place:')\n",
    "print('\\tx.exp_():\\t\\t', x.exp_().numpy())\n",
    "print('\\tx after x.exp_():\\t', x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.]\n",
      " [ 4.  6.]]\n",
      "[[ 0.  2.]\n",
      " [ 4.  6.]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4).view(2, 2)\n",
    "y = torch.arange(4, 8).view(2, 2)\n",
    "z = torch.arange(8, 12).view(2, 2)\n",
    "\n",
    "# Not-in-place:\n",
    "u = x + 2 * y - z    # 3 array allocations?\n",
    "print(u.numpy())\n",
    "\n",
    "# In-place\n",
    "u = y.clone()        # 1 array allocation\n",
    "u.mul_(2)\n",
    "u.add_(x)\n",
    "u.sub_(z)\n",
    "print(u.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting на pytorch (аналогично numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n",
      "b: \n",
      " 1  0  1  0\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "a + b: \n",
      " 2  1  2  1\n",
      " 2  1  2  1\n",
      " 2  1  2  1\n",
      " 3  2  3  2\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "c: \n",
      " 0.1850 -0.1429 -1.0659  0.3823\n",
      " 0.9214 -1.2254 -0.0475 -0.1747\n",
      "-0.8821 -0.6159  1.1399 -2.3033\n",
      " 2.1796  0.0658  3.3187 -2.2893\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "b + c: \n",
      " 1.1850 -0.1429 -0.0659  0.3823\n",
      " 1.9214 -1.2254  0.9525 -0.1747\n",
      " 0.1179 -0.6159  2.1399 -2.3033\n",
      " 3.1796  0.0658  4.3187 -2.2893\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 1, 1, 2]).view(4, 1)\n",
    "b = torch.Tensor([1, 0, 1, 0]).view(1, 4)\n",
    "c = torch.randn(16).view(4, 4)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('a + b:', a + b)\n",
    "print('c:', c)\n",
    "print('b + c:', b + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более подробную информацию можно найти на http://pytorch.org/docs/master/notes/broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с тензорами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано 100 объектов, каждый из которых описывается 10-мерным вектором, и 5 точек, каждая из которых также задается 10-мерным вектором. Объекты лежат в матрице X, точки – в матрице Y.\n",
    "\n",
    "Надо для каждого объекта из X найти индекс ближайшей точки из Y только с помощью операций над тензорами\n",
    "(нельзя использовать циклы, list comprehensions, рекурсию, etc,\n",
    "потому что решение с ними будет работать в несколько раз или на несколько порядков медленнее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(100, 10)\n",
    "Y = torch.randn(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение с семинара:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 1, 2, 2, 2, 2, 3, 2, 0,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 4, 2, 2,\n",
       "       2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 0, 4, 4, 2, 1, 3, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 4, 2, 2, 2, 0, 2, 2, 2, 2, 3, 2,\n",
       "       2, 2, 2, 2, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X.view(100, 1, 10) - Y.view(1, 5, 10)) ** 2).sum(dim=-1).min(dim=-1)[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это решение плохо тем, что в качестве промежуточного результата вычилений в нем присутствует трехмерный тензор,\n",
    "который занимает $O(NMD)$ памяти, где N – число объектов, M – число точек, D – размерность пространства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внимание, задача!\n",
    "Утверждается, что есть другое решение с такой же скоростью работы,\n",
    "но использующее $O(NM)$ памяти для результатов промежуточных вычислений.\n",
    "Предлагается найти его.\n",
    "\n",
    "Подсказка: найти матрицу попарных скалярных произведений между объектами\n",
    "и точками можно с помощью одного матричного умножения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#c**2 = a**2 + b**2 -2abcos(a), 2abcos(a) - scalar multiplication of a&b\n",
    "XY_scalar = torch.matmul(X, Y.transpose(0, 1))\n",
    "X_2 = (X**2).sum(dim=1)\n",
    "Y_2 = (Y**2).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y_2.view(1, 5)+X_2.view(100, 1) - XY_scalar).min(dim=1)[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "`x.cuda()` копирует тензор на GPU и возвращает объект, соответствующий этому скопированному тензору.\n",
    "Можно явно указать номер GPU, на который нужно скопировать тензор: `x.cuda(gpu_id)`.\n",
    "Если тензор уже лежал на нужном GPU, то возвращается сам тензор, копирования не производится.\n",
    "Аналогично работает `x.cpu()`. \n",
    "\n",
    "Операции можно осуществлять только над тензорами, лежащими на одном устройстве.\n",
    "Нарушение этого правила приводит к ошибке.\n",
    "Результат операции находится на том же устройстве, что и операнды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor vs Variable\n",
    "\n",
    "http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "`Variable` – обертка над Tensor для использования в вычислительных графах. Позволяет вычислять градиенты автоматически.\n",
    "\n",
    "Tensor и Variable конвертируются друг в друга:\n",
    "```\n",
    "tensor to variable: Variable(x)\n",
    "variable to tensor: x.data\n",
    "```\n",
    "\n",
    "Нельзя смешивать Tensor и Variable в одной операции.\n",
    "\n",
    "Некоторые операции могут работать только с тензорами, некоторые только с переменными (torch.nn.functional.whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "sequence = torch.randn(1, 8, 10)\n",
    "filters = torch.randn(2, 8, 3)\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of Variables:\n",
      "Variable containing:\n",
      "-0.9251\n",
      " 0.4932\n",
      " 0.1173\n",
      " 1.4256\n",
      " 0.4002\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# works:\n",
    "print('sum of Variables:')\n",
    "print(Variable(a) + Variable(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of Variable and Tensor:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "add() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8bcdd0d9aead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# will not work:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sum of Variable and Tensor:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: add() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n"
     ]
    }
   ],
   "source": [
    "# will not work:\n",
    "print('sum of Variable and Tensor:')\n",
    "print(Variable(a) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works:\n",
    "print('conv1d over Variables:')\n",
    "print(torch.nn.functional.conv1d(Variable(sequence), Variable(filters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will not work\n",
    "print(\"conv1d (tensors):\")\n",
    "print(torch.nn.functional.conv1d(sequence, filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic gradients\n",
    "\n",
    "Автоматическое вычисление градиентов:\n",
    "\n",
    "1. Создать переменную: `a = Variable(..., requires_grad=True)`\n",
    "\n",
    "2. Определить какую-нибудь дифференцируемую _скалярную_ функцию `loss = whatever(a)`\n",
    "\n",
    "3. Запросить обратный проход `loss.backward()`\n",
    "\n",
    "4. Градиенты будут доступны в `a.grads`\n",
    "\n",
    "\n",
    "Есть два важных отличия Pytorch от Theano/TF:\n",
    "\n",
    "1. Функцию ошибки можно изменять динамически, например на каждом минибатче.\n",
    "\n",
    "2. После вычисления `.backward()` градиенты сохраняются в `.grad` каждой задействованной переменной, при повторных вызовах градиенты суммируются. Это позволяет использовать несколько функций ошибок или виртуально увеличивать batch_size. Поэтому после каждого шага оптимизатора градиенты стоит обнулять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простой пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_tensor = torch.randn(4)\n",
    "y_tensor = torch.randn(4)\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = Variable(y_tensor, requires_grad=True)\n",
    "z = x * y + 10\n",
    "p = z.sum()\n",
    "p.backward()\n",
    "print('x:', x.data)\n",
    "print('y:', y.data)\n",
    "print('dp / dx:', x.grad.data)\n",
    "print('dp / dy:', y.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Variable(torch.randn(4).view(1, 4), requires_grad=True)\n",
    "w = Variable(torch.randn(4).view(4, 1), requires_grad=True)\n",
    "b = 7\n",
    "T = torch.mm(X, w) + b\n",
    "Z = torch.sigmoid(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.grad.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.grad.zero_(), w.grad.zero_(), X.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнуление градиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.Tensor([1, 1, 1, 1])\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = x ** 2\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "x.data -= 0.5\n",
    "y = 1 / x\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.Tensor([1, 1, 1, 1])\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = x ** 2\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "x.grad.detach_()       # extracting gradient Variable from the previous computational graph (optional)\n",
    "x.grad.data.zero_()    # zero gradinents\n",
    "x.data -= 0.5\n",
    "y = 1 / x\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf vs Non-leaf Variable\n",
    "\n",
    "Градиенты будут сохранены и доступны для использования только для `leaf-variable`.\n",
    "Такое поведение по умолчанию сделано ради экономии памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = x + 1                                         # not a leaf variable\n",
    "p = y.sum()                                       # not a leaf variable\n",
    "p.backward()\n",
    "print('x.data:', x.data.numpy())\n",
    "print('y.data:', y.data.numpy())\n",
    "print('p.data:', p.data.numpy())\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad)\n",
    "print('p.grad:', p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad.data.numpy())\n",
    "print('z.grad:', z.grad)\n",
    "print('p.grad:', p.grad)\n",
    "print('x.is_leaf:', x.is_leaf)\n",
    "print('y.is_leaf:', y.is_leaf)\n",
    "print('z.is_leaf:', z.is_leaf)\n",
    "print('p.is_leaf:', p.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Листовые вершины без градиентов\n",
    "Листовые вершины, в которых не требуется вычислять градиент, создаются с помощью `Variable(..., requires_grad=False)`.\n",
    "Для корректного вызова `.backward()` требуется, чтобы хотя бы для одной листовой вершины требовался градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad)\n",
    "print('z.grad:', z.grad)\n",
    "print('p.grad:', p.grad)\n",
    "print('x.is_leaf:', x.is_leaf)\n",
    "print('y.is_leaf:', y.is_leaf)\n",
    "print('z.is_leaf:', z.is_leaf)\n",
    "print('p.is_leaf:', p.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что для вычисления градиента нужно, чтобы хотя бы одна листовая вершина графа вычисления функции\n",
    "имела `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will not work:\n",
    "x = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиенты промежуточных вершин\n",
    "Для промежуточных вершин мы можем запросить сохранение градиентов с помощью функции `.retain_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "p = y.sum()\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "p.backward()\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "print('dp / dw:', w.grad.data.numpy())\n",
    "print('dp / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что даже при наличии в графе вычислений не-листовых вершин, требующих вычисления градиентов,\n",
    "`.backward()` выдает ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will not work\n",
    "x = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "p = y.sum()\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "p.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отстреливаем себе ноги (НЕ НАДО так делать)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертировать Variable в Tensor и обратно:\n",
    "backward pass не проходит через Tensor, даже если он был сконвертирован из другого Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x out of the computational graph\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(x.data * 2, requires_grad=True)   # the bad conversion is here\n",
    "z = 3 * y + 1\n",
    "p = z.mean()\n",
    "p.backward()\n",
    "print('dp / dx:', x.grad)\n",
    "print('dp / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять размерность тензоров в Variable, но не обнулять градиенты (`.grad.zero_()` сохраняет размер, `.grad = None` не сохраняет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = x * y + 1\n",
    "z.sum().backward()\n",
    "print('dz / dx:', x.grad.data.numpy())\n",
    "print('dz / dy:', y.grad.data.numpy())\n",
    "\n",
    "x.grad.zero_()\n",
    "z.grad.zero_()\n",
    "#x.grad = None\n",
    "#z.grad = None\n",
    "\n",
    "x.data = torch.Tensor([1, 2, 3])\n",
    "y.data = torch.Tensor([1, 2, 3])\n",
    "z = x * y + 1\n",
    "z.sum().backward()\n",
    "print('dz / dx:', x.grad.data.numpy())\n",
    "print('dz / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять значения Variable после вычисления каких-то других выражений с ним и рассчитывать,\n",
    "что градиент от тех выражений будет учитывать новое значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = y ** 2\n",
    "\n",
    "z.data = torch.Tensor([1, 2, 3, 4])  # changing .data before computation matters\n",
    "p = x * z\n",
    "x.data = torch.Tensor([1, 1, 1])     # changing .data after computation doesn't affect gradients\n",
    "\n",
    "p.sum().backward()\n",
    "print('d p_sum / dx:', x.grad.data.numpy())\n",
    "print('d p_sum / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тысячи способов прострелить себе ногу, если использовать механизм автоматического дифференцирования\n",
    "любым другим нетрадиционным образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(50, 10)\n",
    "b = torch.randn(2)\n",
    "W = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наркоманская функция потерь, вынуждающая линейное преобразование переводить точки из многомерного пространства в двумерное на единичную окружность. Для оптимизации использовать градиентный спуск по параметрам преобразования.\n",
    "\n",
    "Линейное преобразование точки $x$ из десятимерного пространства в точку $y$ двумерного пространства с весами преобразования $W$ и $b$:\n",
    "$$y = Wx + b$$\n",
    "\n",
    "Норма в двумерном пространстве – евклидова:\n",
    "$$||y||_2 = \\sqrt{y_1^2 + y_2^2}$$\n",
    "\n",
    "Функция потерь $f_0$ штрафует расстояние от получившейся точки $y$ до единичной окружности:\n",
    "$$f_0(x, W, b) = 0.5 \\cdot \\big| ||y||_2 - 1 \\big| + \\big( ||y||_2 - 1 \\big)^2$$\n",
    "\n",
    "К сожалению, оптимизация функции $f_0$ по $W$ и $b$ может быть проведена аналитически\n",
    "и приводит к тривиальному решению $W = 0$, $b = (1, 0)$.\n",
    "Чтобы избежать такого решения, вводим штраф на близость получившейся точки к вектору $b$, который обращается в 0, если расстояние до вектора $b$ более 1:\n",
    "$$f_1(x, W, b) = \\max\\big(0, \\frac{1}{||y - b||_2} - 1\\big)$$\n",
    "\n",
    "Итоговая функция потерь:\n",
    "$$f(x, W, b) = f_0(x, W, b) + f_1(x, W, b)$$\n",
    "\n",
    "Нужно решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{N}\\sum\\limits_{i = 1}^N f(x_i, W, b) \\to \\min\\limits_{W, b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(X, W, b):\n",
    "    # your code here\n",
    "    Y = torch.matmul(X, W) + b\n",
    "    y_norm = torch.sqrt((Y**2).sum(dim=1))\n",
    "    f_0 = 0.5*torch.abs(y_norm-1) + (y_norm-1)**2\n",
    "    y_b_norm = torch.sqrt(torch.sum((Y - b)**2, dim=1))\n",
    "    f_1 = torch.max(Variable(torch.zeros(50, 1)), 1/y_b_norm - 1)\n",
    "    L = f_0 + f_1 \n",
    "    Q = L.sum()/(L.shape[0]*L.shape[1])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7d3207004357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvariable_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvariable_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvariable_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "variable_X = Variable(X)\n",
    "variable_W = Variable(W, requires_grad=True)\n",
    "variable_b = Variable(b, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variable_W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3d0f72ac0a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'variable_W' is not defined"
     ]
    }
   ],
   "source": [
    "print(f(variable_X, variable_W, variable_b).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variable_W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8afc49c976ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvariable_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_b\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.003\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'variable_W' is not defined"
     ]
    }
   ],
   "source": [
    "opt = Adam([variable_W, variable_b], lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-794ba8d65ce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "num_epoch = 10000\n",
    "for i in range(num_epoch):\n",
    "    opt.zero_grad()\n",
    "    loss = f(variable_X, variable_W, variable_b)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-9c7d9083178b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1eb910418db2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d4ae2bdaa0b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariable_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_W\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = variable_X.mm(variable_W).add(variable_b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время писать нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: нейросети крайне плохо обучаются, если подаваемые им на вход значения велики по модулю.\n",
    "Поэтому перед обучением нейросети каждый признак независимо нормируют\n",
    "(исключение – сверточные нейросети, там нормируют изображение поканально, а не попиксельно, но об этом потом).\n",
    "\n",
    "Можно использовать разные нормировки.\n",
    "Наиболее популярно вычитать среднее и делить на дисперсию (нужно внимательно подходить к этому методу,\n",
    "когда выборочная дисперсия мала или равна нулю, и обрабатывать такие случаи отдельно).\n",
    "Можно также вычитать медиану и делить на интерквартильный размах, масштабировать все данные в отрезок $[-1, 1]$, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно реализовать свою нормировку данных здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "mean = X_train.mean(dim=0)\n",
    "std = X_train.std(dim=0)\n",
    "std.apply_(lambda x: x if x>0 else 0.0001)\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем слои нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        self.children = []\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns list of parameters of module and its children.\"\"\"\n",
    "        res = []\n",
    "        for submodule in self.children:\n",
    "            res += submodule.parameters()\n",
    "        for param in res:\n",
    "            if not isinstance(param, Variable):\n",
    "                raise Exception('Parameters must be Variables.')\n",
    "        return res\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()   # detachs gradient Variable from the computational graph\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Sets module into train mode (for DropOut, BatchNorm, etc).\"\"\"\n",
    "        self.training = True\n",
    "        for submodule in self.children:\n",
    "            submodule.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets module into evaluation mode.\"\"\"\n",
    "        self.training = False\n",
    "        for submodule in self.children:\n",
    "            submodule.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Module):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = Variable(torch.randn(input_units, output_units)*0.00001, requires_grad=True)  # your code here\n",
    "        self.biases = Variable(torch.randn(1, output_units)*0.00001, requires_grad=True)   # your code here\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.output = torch.matmul(input, self.weights) + self.biases\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs.\"\"\"\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []  # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies elementwise ReLU to [batch, num_units] Variable matrix.\"\"\"\n",
    "        # your code here\n",
    "        self.output = torch.nn.functional.relu(input)\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogSoftmax(Module):\n",
    "    def __init__(self):\n",
    "        super(LogSoftmax, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies softmax to each row and then applies component-wise log.\n",
    "        Input shape:  [batch, num_units] (Variable)\n",
    "        Output shape: [batch, num_units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.output = torch.nn.functional.log_softmax(input, dim=1)\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyNetwork(Module):\n",
    "    def __init__(self, input_size, hidden_layers_size, hidden_layers_number, output_size):\n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        network = []\n",
    "        network.append(Dense(input_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "        for i in range(hidden_layers_number - 1):\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "        network.append(Dense(hidden_layers_size, output_size))\n",
    "        network.append(LogSoftmax())\n",
    "\n",
    "        self.children = network\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies all layers of neural network to the input.\n",
    "        Input shape:  [batch, num_units] (Variable)\n",
    "        Output shape: [batch, num_units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        output = input\n",
    "        for layer in self.children:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"Returns negative log-likelihood of target under model\n",
    "    represented by activations (log probabilities of classes).\n",
    "    Activations shape: [batch, num_classes] (Variable)\n",
    "    Target shape:      [batch]              (Variable)\n",
    "    Output shape: 1 (scalar, Variable)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    #loss = - torch.sum(activations[:, target])\n",
    "    loss = - torch.sum(activations[np.arange(len(target)), target])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.data -= self.learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(dataset, network, prefix='Test loss:', optimizer=None):\n",
    "    # Change mode for all layers.\n",
    "    if optimizer:\n",
    "        network.train()\n",
    "    else:\n",
    "        network.eval()\n",
    "\n",
    "    batch_size = 100\n",
    "    batchgenerator = torch.utils.data.DataLoader(dataset, batch_size, True)\n",
    "\n",
    "    avg_loss = 0\n",
    "    for i, (batch_data, batch_target) in enumerate(batchgenerator):\n",
    "        batch_output = network.forward(Variable(batch_data))\n",
    "        batch_loss = crossentropy(batch_output, Variable(batch_target))\n",
    "        #print(batch_output)\n",
    "        batch_loss.backward()\n",
    "        batch_loss = batch_loss.data.numpy()[0]\n",
    "        avg_loss += (batch_loss - avg_loss) / (i + 1)\n",
    "        if optimizer:\n",
    "            optimizer.step()\n",
    "            network.zero_grad()\n",
    "    print(prefix, avg_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 221.693268367\n",
      "Test loss: 207.356959534\n",
      "Train loss: 221.709107535\n",
      "Test loss: 207.325334167\n",
      "Train loss: 215.913970947\n",
      "Test loss: 184.784657288\n",
      "Train loss: 128.114058086\n",
      "Test loss: 78.4177284241\n",
      "Train loss: 45.9722008024\n",
      "Test loss: 32.0823406219\n",
      "Train loss: 24.3204667909\n",
      "Test loss: 18.1917114258\n",
      "Train loss: 15.4498525006\n",
      "Test loss: 11.9230111122\n",
      "Train loss: 11.4012275764\n",
      "Test loss: 9.33497753143\n",
      "Train loss: 9.12822192056\n",
      "Test loss: 7.59143161774\n",
      "Train loss: 7.5957518816\n",
      "Test loss: 6.53410797119\n",
      "Train loss: 6.67689425605\n",
      "Test loss: 5.52004499435\n",
      "Train loss: 5.76055191244\n",
      "Test loss: 4.96747469902\n",
      "Train loss: 5.12720776456\n",
      "Test loss: 4.31735992432\n",
      "Train loss: 4.49048909119\n",
      "Test loss: 3.93370199203\n",
      "Train loss: 4.01060628039\n",
      "Test loss: 3.51529927254\n",
      "Train loss: 3.54166832141\n",
      "Test loss: 3.1078253746\n",
      "Train loss: 3.22932979039\n",
      "Test loss: 2.78191628456\n",
      "Train loss: 2.89354721563\n",
      "Test loss: 2.54150979519\n",
      "Train loss: 2.56870407292\n",
      "Test loss: 2.28947069645\n",
      "Train loss: 2.42793481691\n",
      "Test loss: 2.08639733791\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "sgd = SGDOptimizer(network.parameters(), 0.005)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', sgd)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Больше оптимизаторов Б-гу Оптимизации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDMomentumOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, momentum=0.9):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        # your code here\n",
    "        self.lag_parameters = self.parameters\n",
    "        \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        previous = self.parameters \n",
    "        for param, lag_param in zip(self.parameters, self.lag_parameters):\n",
    "            param.data -= self.learning_rate * param.grad.data + self.momentum*(param.data-lag_param.data)\n",
    "            self.lag_parameters = previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RMSPropOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta=0.9, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        self.v = [Variable(torch.zeros(tensor.size()), requires_grad=True) for tensor in parameters]\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        for param, v in zip(self.parameters, self.v):\n",
    "            v.data = self.beta*v.data + (1-self.beta)*(param.grad.data)**2 \n",
    "            param.data -= self.learning_rate*param.grad.data/torch.sqrt(v.data + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        self.m = [Variable(torch.zeros(tensor.size())) for tensor in parameters]\n",
    "        self.v = [Variable(torch.zeros(tensor.size())) for tensor in parameters]\n",
    "        self.t = 0\n",
    "        \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.t += 1\n",
    "        for param, m, v in zip(self.parameters, self.m, self.v):\n",
    "            m.data = self.beta1 * m.data + (1 - self.beta1) * param.grad.data\n",
    "            v.data = self.beta2 * v.data + (1 - self.beta2) * param.grad.data**2\n",
    "            m_hat = m.data / (1 - self.beta1**self.t)\n",
    "            v_hat = v.data / (1 - self.beta2**self.t)\n",
    "            param.data -= (self.learning_rate / (torch.sqrt(v_hat) + self.eps)) * m_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 221.792132786\n",
      "Test loss: 207.497396851\n",
      "Train loss: 208.627980913\n",
      "Test loss: 142.205712891\n",
      "Train loss: 63.789528438\n",
      "Test loss: 28.9509429932\n",
      "Train loss: 18.8545825141\n",
      "Test loss: 14.9617086411\n",
      "Train loss: 11.3087986708\n",
      "Test loss: 9.23672075272\n",
      "Train loss: 8.88867645604\n",
      "Test loss: 8.56956825256\n",
      "Train loss: 10.5502818993\n",
      "Test loss: 11.6600829124\n",
      "Train loss: 5.79795605796\n",
      "Test loss: 4.49802207947\n",
      "Train loss: 3.85666584117\n",
      "Test loss: 3.96704499722\n",
      "Train loss: 3.26321063723\n",
      "Test loss: 4.8798538208\n",
      "Train loss: 2.9778298395\n",
      "Test loss: 2.25139176846\n",
      "Train loss: 2.11880375658\n",
      "Test loss: 1.9001418829\n",
      "Train loss: 1.66512248346\n",
      "Test loss: 1.57926323414\n",
      "Train loss: 1.44689011148\n",
      "Test loss: 1.42686934471\n",
      "Train loss: 1.26057777234\n",
      "Test loss: 1.22988757491\n",
      "Train loss: 1.1117405636\n",
      "Test loss: 1.08971599936\n",
      "Train loss: 0.988719438868\n",
      "Test loss: 0.948154628277\n",
      "Train loss: 0.89676123006\n",
      "Test loss: 0.906381630898\n",
      "Train loss: 0.829638031977\n",
      "Test loss: 0.781771972775\n",
      "Train loss: 0.752192045961\n",
      "Test loss: 0.726988387108\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = SGDMomentumOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 124.717394829\n",
      "Test loss: 44.1945404053\n",
      "Train loss: 27.9040530069\n",
      "Test loss: 19.4143827438\n",
      "Train loss: 14.9262724604\n",
      "Test loss: 12.0128164291\n",
      "Train loss: 9.42871136325\n",
      "Test loss: 7.67577753067\n",
      "Train loss: 5.96195571763\n",
      "Test loss: 5.70320343971\n",
      "Train loss: 4.07281899452\n",
      "Test loss: 4.07001588345\n",
      "Train loss: 2.82239182506\n",
      "Test loss: 3.31514434814\n",
      "Train loss: 1.86271859067\n",
      "Test loss: 2.17463693023\n",
      "Train loss: 1.27579508881\n",
      "Test loss: 1.67005878687\n",
      "Train loss: 0.902630478144\n",
      "Test loss: 1.4451825738\n",
      "Train loss: 0.682061655181\n",
      "Test loss: 0.992381381989\n",
      "Train loss: 0.336626931493\n",
      "Test loss: 0.818373528123\n",
      "Train loss: 0.235988681072\n",
      "Test loss: 0.711748760939\n",
      "Train loss: 0.211213894721\n",
      "Test loss: 0.634506493807\n",
      "Train loss: 0.294234647815\n",
      "Test loss: 0.171560300887\n",
      "Train loss: 0.0915165645336\n",
      "Test loss: 0.094993891567\n",
      "Train loss: 0.0591958463857\n",
      "Test loss: 0.0604913912714\n",
      "Train loss: 0.0530295801083\n",
      "Test loss: 0.0739043958485\n",
      "Train loss: 0.031776202643\n",
      "Test loss: 0.039821170643\n",
      "Train loss: 0.0659443444373\n",
      "Test loss: 0.0257660945877\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = RMSPropOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 180.333901269\n",
      "Test loss: 96.9577812195\n",
      "Train loss: 65.6407427107\n",
      "Test loss: 42.5596565247\n",
      "Train loss: 30.3924466542\n",
      "Test loss: 21.1837905884\n",
      "Train loss: 15.3301186562\n",
      "Test loss: 11.2946683884\n",
      "Train loss: 9.17791019167\n",
      "Test loss: 7.36945893764\n",
      "Train loss: 6.34908700841\n",
      "Test loss: 5.56227004528\n",
      "Train loss: 4.66237752778\n",
      "Test loss: 4.34610834122\n",
      "Train loss: 3.4843770521\n",
      "Test loss: 3.63975114822\n",
      "Train loss: 2.66346748812\n",
      "Test loss: 3.06203801632\n",
      "Train loss: 2.10765596798\n",
      "Test loss: 2.50851314068\n",
      "Train loss: 1.68493120585\n",
      "Test loss: 2.13663833141\n",
      "Train loss: 1.41378126826\n",
      "Test loss: 1.86096903086\n",
      "Train loss: 1.15184976799\n",
      "Test loss: 1.59302366972\n",
      "Train loss: 0.987093228315\n",
      "Test loss: 1.44169704914\n",
      "Train loss: 0.851644066828\n",
      "Test loss: 1.28307608962\n",
      "Train loss: 0.731254270034\n",
      "Test loss: 1.20932716131\n",
      "Train loss: 0.62611842049\n",
      "Test loss: 1.0868888855\n",
      "Train loss: 0.542785914881\n",
      "Test loss: 1.01632710099\n",
      "Train loss: 0.487751379609\n",
      "Test loss: 0.92935885191\n",
      "Train loss: 0.435861863196\n",
      "Test loss: 0.873154050112\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = AdamOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот пункт обязателен к выполнению.\n",
    "Для того, чтобы получить бонусный балл за этот пункт, нужно эффективно реализовать DropOut:\n",
    "не вычислять активации выкинутых нейронов, прежде чем их обнулить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseWithDropOut(Module):\n",
    "    def __init__(self, input_units, output_units, dropout_rate, nonlinearity):\n",
    "        \"\"\"A dense layer is a layer which performs a learned\n",
    "        affine transformation and applies dropout:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        \"\"\"\n",
    "        super(DenseWithDropOut, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = nonlinearity\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = Variable(torch.randn(input_units, output_units)*0.0001, requires_grad=True) # your code here\n",
    "        self.biases = Variable(torch.randn(1, output_units)*0.0001, requires_grad=True) # your code here\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input, idxs_x):\n",
    "        \"\"\"Performs an affine transformation with dropout.\n",
    "        In training mode:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        In evaluation mode:\n",
    "        f(x) = g(W x + b) (1 - p)\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        '''\n",
    "        Не работает быстрее\n",
    "        if self.training:\n",
    "            y = np.ones(self.output_units)\n",
    "            num_zeroes = np.random.binomial(self.output_units, self.dropout_rate)\n",
    "            idxs = np.random.choice(self.output_units, num_zeroes, replace=False)\n",
    "            y[idxs] = 0\n",
    "            y = y.reshape((1, self.output_units))\n",
    "            y = Variable(torch.from_numpy(y))\n",
    "            output = self.nonlinearity(torch.matmul(input, self.weights) + self.biases)\n",
    "            output = output.float()*y.float() \n",
    "        '''\n",
    "        if self.training:\n",
    "            \"\"\"В данном форварде дропаут реализован так, что на вход в следующий слой помимо output\n",
    "               передаются также индексы весов, на которые нам надо домножать инпут. Скорость работы\n",
    "               получается больше, чем если восстанавливать нули во всей матрице после дропаута.\n",
    "            \"\"\"\n",
    "            y = np.ones(self.output_units)\n",
    "            num_zeroes = np.random.binomial(self.output_units, 1-self.dropout_rate)\n",
    "            idxs = np.random.choice(self.output_units, num_zeroes, replace=False)\n",
    "            if num_zeroes is 0:\n",
    "                idxs = np.array([0])\n",
    "            idxs = torch.from_numpy(idxs)\n",
    "            idxs = Variable(idxs).long()\n",
    "            weights_remain = self.weights.index_select(1, idxs)\n",
    "            biases_remain = self.biases.index_select(1, idxs)\n",
    "            weights_remain = weights_remain.index_select(0, idxs_x.long())\n",
    "            print('Input shape is: ', input.shape, 'Weights shape is: ', weights_remain.shape)\n",
    "            output = [self.nonlinearity(torch.matmul(input, weights_remain) + biases_remain), idxs]\n",
    "        else:\n",
    "            output = self.nonlinearity(torch.matmul(input, self.weights) + self.biases)*(1-self.dropout_rate)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, верно ли, что полносвязная сеть с dropout работает быстрее, чем обычная полносвязная сеть, поскольку на каждом проходе вычисляются произведения матриц меньшего размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 203])\n",
      "Input shape is:  torch.Size([10000, 203]) Weights shape is:  torch.Size([203, 232])\n",
      "Input shape is:  torch.Size([10000, 232]) Weights shape is:  torch.Size([232, 195])\n",
      "Input shape is:  torch.Size([10000, 195]) Weights shape is:  torch.Size([195, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 206])\n",
      "Input shape is:  torch.Size([10000, 206]) Weights shape is:  torch.Size([206, 205])\n",
      "Input shape is:  torch.Size([10000, 205]) Weights shape is:  torch.Size([205, 209])\n",
      "Input shape is:  torch.Size([10000, 209]) Weights shape is:  torch.Size([209, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 225])\n",
      "Input shape is:  torch.Size([10000, 225]) Weights shape is:  torch.Size([225, 193])\n",
      "Input shape is:  torch.Size([10000, 193]) Weights shape is:  torch.Size([193, 208])\n",
      "Input shape is:  torch.Size([10000, 208]) Weights shape is:  torch.Size([208, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 191])\n",
      "Input shape is:  torch.Size([10000, 191]) Weights shape is:  torch.Size([191, 220])\n",
      "Input shape is:  torch.Size([10000, 220]) Weights shape is:  torch.Size([220, 186])\n",
      "Input shape is:  torch.Size([10000, 186]) Weights shape is:  torch.Size([186, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 191])\n",
      "Input shape is:  torch.Size([10000, 191]) Weights shape is:  torch.Size([191, 203])\n",
      "Input shape is:  torch.Size([10000, 203]) Weights shape is:  torch.Size([203, 195])\n",
      "Input shape is:  torch.Size([10000, 195]) Weights shape is:  torch.Size([195, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 201])\n",
      "Input shape is:  torch.Size([10000, 201]) Weights shape is:  torch.Size([201, 195])\n",
      "Input shape is:  torch.Size([10000, 195]) Weights shape is:  torch.Size([195, 216])\n",
      "Input shape is:  torch.Size([10000, 216]) Weights shape is:  torch.Size([216, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 225])\n",
      "Input shape is:  torch.Size([10000, 225]) Weights shape is:  torch.Size([225, 188])\n",
      "Input shape is:  torch.Size([10000, 188]) Weights shape is:  torch.Size([188, 197])\n",
      "Input shape is:  torch.Size([10000, 197]) Weights shape is:  torch.Size([197, 1])\n",
      "Input shape is:  torch.Size([10000, 2000]) Weights shape is:  torch.Size([2000, 194])\n",
      "Input shape is:  torch.Size([10000, 194]) Weights shape is:  torch.Size([194, 203])\n",
      "Input shape is:  torch.Size([10000, 203]) Weights shape is:  torch.Size([203, 205])\n",
      "Input shape is:  torch.Size([10000, 205]) Weights shape is:  torch.Size([205, 1])\n",
      "519 ms ± 35.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.03 s ± 29.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "width = 2000\n",
    "network1 = [\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, 1, 0, lambda x: x)\n",
    "]\n",
    "network2 = [\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, 1)\n",
    "]\n",
    "X = torch.randn(10000, width)\n",
    "\n",
    "# check whether DenseWithDropOut works faster than Dense\n",
    "def test_network(network):\n",
    "    x = Variable(X)\n",
    "    idxs_x = Variable(torch.from_numpy(np.arange(0, x.shape[1], 1))).long()\n",
    "    for layer in network:\n",
    "        if isinstance(x, list):\n",
    "            x = x[0]\n",
    "            idxs_x = x[1]   \n",
    "        x = layer.forward(x, idxs_x)\n",
    "    if isinstance(x, list):\n",
    "        x[0].mean().backward()\n",
    "    else:\n",
    "        x.mean().backward()\n",
    "\n",
    "    for layer in network:\n",
    "        x = layer.zero_grad()\n",
    "\n",
    "def test_network_2(network):\n",
    "    x = Variable(X)\n",
    "    for layer in network:\n",
    "        x = layer.forward(x)\n",
    "    x.mean().backward()\n",
    "    for layer in network:\n",
    "        x = layer.zero_grad()\n",
    "\n",
    "%timeit test_network(network1)\n",
    "%timeit test_network_2(network2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более узких слоев, меньших dropout rate и меньших размеров батча увеличение производительности не настолько существенно или может вообще отсутствовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
